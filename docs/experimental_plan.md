# Experimental Plan: Evaluating RAG-SLAM

This document outlines the experimental plan to evaluate the performance of a semantic loop-closure mechanism (RAG-SLAM) against state-of-the-art baselines.

## 1. Experimental Setup

### 1.1. Datasets

We will use the following datasets for our evaluation:
*   **TUM RGB-D Dataset:**
    *   `freiburg1_xyz`: A simple sequence for initial validation.
    *   `freiburg2_desk_with_person`: To test robustness to dynamic objects.
*   **MetaSpatial Synthetic Dataset:** A large-scale synthetic dataset generated using Blender on Google Cloud. This dataset will contain a diverse range of plausible and impossible scenes to rigorously test the physics-aware pre-training.

### 1.2. Systems for Comparison

We will compare three SLAM system configurations:
*   **System A (Baseline 1):** **ORB-SLAM3 with Semantic Features**. This provides a strong baseline that combines geometric and semantic information.
*   **System B (Baseline 2):** **Kimera**. This is a full semantic SLAM system that builds a 3D mesh of the environment and labels objects with semantic classes.
*   **System C (Ours):** **ORB-SLAM3** augmented with our **Gemini-based semantic RAG pipeline**.

## 2. Evaluation Metrics

### 2.1. Primary Metric: Absolute Trajectory Error (ATE)

The primary metric for evaluating the performance of the three systems will be the **Absolute Trajectory Error (ATE)**.

*   **Definition:** ATE measures the global consistency of the estimated trajectory by comparing it to the ground truth trajectory. It is calculated as the root-mean-square error (RMSE) over the differences between the positions of the estimated and ground truth trajectories after they have been aligned.
*   **Justification:** ATE is a suitable metric for this experiment because it directly reflects the accumulated drift of the SLAM system over time. A successful loop closure significantly reduces drift, which will be clearly visible as a lower ATE value.

## 3. Methodology

The experiment will be conducted as follows:

1.  **Dataset Preparation:**
    *   Download the selected sequences from the TUM RGB-D dataset website.
    *   Generate the MetaSpatial synthetic dataset using Blender on Google Cloud.
    *   For each keyframe generated by the SLAM system, a caption will be generated using a pre-trained vision-to-text model.

2.  **System Execution:**
    *   **Run System A (Baseline 1):** Execute ORB-SLAM3 with semantic features on the selected sequences. The estimated trajectories will be saved to files in the TUM trajectory file format.
    *   **Run System B (Baseline 2):** Execute Kimera on the selected sequences. The estimated trajectories will be saved to files in the TUM trajectory file format.
    *   **Run System C (Ours):** Execute the modified ORB-SLAM3 (with the RAG pipeline) on the same sequences. The estimated trajectories will also be saved in the TUM format.

3.  **Metric Computation:**
    *   Use a standard SLAM evaluation tool (e.g., the `evo` Python package) to compute the Absolute Trajectory Error (ATE) for all three systems on all sequences.

4.  **Data Analysis:**
    *   Compare the ATE values obtained for all systems.
    *   Plot the trajectories of all systems against the ground truth to visually inspect the differences.

## 4. Success Criteria

The experiment will be considered a success if **System C (Ours)** demonstrates a statistically significant reduction in **Absolute Trajectory Error (ATE)** compared to both **System A** and **System B**.